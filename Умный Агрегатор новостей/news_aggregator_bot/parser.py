import requests
from bs4 import BeautifulSoup
from typing import List, Tuple, Optional
import logging
import re
import time

logging.basicConfig(level=logging.INFO)

async def parse_news_from_url(url: str, site_id: int = None) -> List[Tuple[str, str, str]]:
    """
    —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        print(f"üîç –Ω–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–∞: {url}")
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        response.encoding = 'utf-8'
        print(f"‚úÖ —Å–∞–π—Ç –∑–∞–≥—Ä—É–∂–µ–Ω, —Å—Ç–∞—Ç—É—Å: {response.status_code}, –∫–æ–¥–∏—Ä–æ–≤–∫–∞: {response.encoding}")

        soup = BeautifulSoup(response.content, 'html.parser')
        news_items = []

        print(f"üîç –ø–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π —Å {url}")

        # –∏—â–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏
        all_links = soup.find_all('a', href=True)
        print(f"üîç –Ω–∞–π–¥–µ–Ω–æ {len(all_links)} —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ")

        found_news_count = 0
        processed_urls = set()  # –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤

        for i, link in enumerate(all_links[:100]):  # –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 100 —Å—Å—ã–ª–æ–∫
            href = link.get('href')
            text = link.get_text(strip=True)

            if not href or not text or len(text) < 5:
                continue

            # –æ—á–∏—â–∞–µ–º –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º url
            href = clean_url(href, url)
            
            # –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
            if href in processed_urls:
                continue
            processed_urls.add(href)

            # –ø—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ —Å—Å—ã–ª–∫–æ–π –Ω–∞ –Ω–æ–≤–æ—Å—Ç—å
            if is_news_link(href, url):
                # –ø—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–æ–≤–æ—Å—Ç–∏
                content = await extract_news_content(href, text)
                
                # –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–µ–Ω—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∫–∞–∫ –∫–æ–Ω—Ç–µ–Ω—Ç
                if not content or len(content) < 10:
                    content = text[:200]

                # –æ—á–∏—â–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                title = re.sub(r'\s+', ' ', text).strip()

                news_items.append((title, href, content))
                found_news_count += 1
                print(f"‚úÖ –Ω–∞–π–¥–µ–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å {found_news_count}: {title[:50]}...")
                print(f"   url: {href}")
                print(f"   content: {content[:100]}...")
                
                if found_news_count >= 15:  # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
                    break

        print(f"üìä –≤—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {found_news_count}")
        print(f"üìä –≤—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {len(news_items)}")

        # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        if site_id is not None:
            try:
                from database import save_news_if_new
                saved_count = 0
                for title, url_item, content in news_items:
                    saved = await save_news_if_new(site_id=site_id, title=title, url=url_item, content=content)
                    if saved:
                        saved_count += 1

                print(f"‚úÖ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {saved_count} –Ω–æ–≤–æ—Å—Ç–µ–π –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö")
            except Exception as e:
                print(f"‚ö†Ô∏è –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö: {e}")
        else:
            print("‚ö†Ô∏è –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: site_id –Ω–µ —É–∫–∞–∑–∞–Ω, –Ω–æ–≤–æ—Å—Ç–∏ –Ω–µ –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö")

        return news_items[:15]

    except Exception as e:
        logging.error(f"‚ùå –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url}: {e}")
        return []

async def extract_news_content(news_url: str, fallback_title: str) -> Optional[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –µ–µ URL
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        # –î–µ–ª–∞–µ–º –ø–∞—É–∑—É –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        time.sleep(1)
        
        response = requests.get(news_url, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = 'utf-8'

        soup = BeautifulSoup(response.content, 'html.parser')
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å–∞–π—Ç–æ–≤
        domain = ''
        try:
            from urllib.parse import urlparse
            domain = urlparse(news_url).netloc
        except:
            pass
        
        content = ""
        
        # –î–ª—è habr.com
        if 'habr.com' in domain:
            # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–∞—Ç—å–∏
            content_selectors = [
                '.article-formatted-body', '.post__text', '.article-formatted-body--full',
                '.post__text-html', '.article__text', '.post-content'
            ]
            for selector in content_selectors:
                elements = soup.select(selector)
                for element in elements:
                    text = element.get_text(strip=True)
                    if len(text) > 100:  # –î–ª—è habr.com —Ç—Ä–µ–±—É–µ–º –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
                        content += " " + text
                        if len(content) > 500:  # –ë–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è habr.com
                            break
                if len(content) > 500:
                    break
        
        # –î–ª—è tass.ru
        elif 'tass.ru' in domain:
            # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–æ–≤–æ—Å—Ç–∏
            content_selectors = [
                '.article__text', '.text', '.article-body', '.news-text',
                '.article__content', '.text-block', '.article__body'
            ]
            for selector in content_selectors:
                elements = soup.select(selector)
                for element in elements:
                    text = element.get_text(strip=True)
                    if len(text) > 50:
                        content += " " + text
                        if len(content) > 400:
                            break
                if len(content) > 400:
                    break
        
        # –î–ª—è –¥—Ä—É–≥–∏—Ö —Å–∞–π—Ç–æ–≤ - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ª–æ–≥–∏–∫–∞
        else:
            content_selectors = [
                'article', '.article', '.content', '.text', '.body',
                '.post-content', '.entry-content', '.news-content',
                '.story', '.story-body', '.article-body', '.post-body',
                'div[class*="content"]', 'div[class*="text"]', 'div[class*="body"]',
                'p', '.lead', '.summary', '.description'
            ]
            
            for selector in content_selectors:
                elements = soup.select(selector)
                for element in elements:
                    text = element.get_text(strip=True)
                    if len(text) > 50:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã
                        content += " " + text
                        if len(content) > 300:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
                            break
                if len(content) > 300:
                    break
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç, –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Ö–æ—Ç—è –±—ã –º–µ—Ç–∞-–æ–ø–∏—Å–∞–Ω–∏–µ
        if not content or len(content) < 30:
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            if meta_desc:
                content = meta_desc.get('content', '')
        
        # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        if not content or len(content) < 10:
            content = fallback_title[:200]
        
        # –û—á–∏—â–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        content = re.sub(r'\s+', ' ', content).strip()
        
        return content[:500] if content else None
        
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è {news_url}: {e}")
        return None

def clean_url(link: str, base_url: str) -> str:
    """–û—á–∏—â–∞–µ—Ç –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç URL"""
    if not link:
        return ""

    link = link.strip()

    # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å—Å—ã–ª–∫–∏
    if any(skip in link.lower() for skip in ['javascript:', 'mailto:', 'tel:', 'data:', '#']):
        return ""

    if link.startswith('http'):
        return link

    from urllib.parse import urljoin, urlparse

    if link.startswith('/'):
        parsed_base = urlparse(base_url)
        return f"{parsed_base.scheme}://{parsed_base.netloc}{link}"

    return urljoin(base_url.rstrip('/') + '/', link.lstrip('./'))

def is_news_link(link: str, base_url: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Å—ã–ª–∫–∞ –Ω–æ–≤–æ—Å—Ç—å—é"""
    if not link:
        return False

    from urllib.parse import urlparse
    parsed = urlparse(link)
    path = parsed.path.lower()

    # –ò—Å–∫–ª—é—á–∞–µ–º —è–≤–Ω–æ –Ω–µ –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏
    exclude_patterns = [
        '/press', '/category', '/tag', '/tags', '/archive', '/page',
        '/author', '/authors', '/search', '/rss', '/feed', '/sitemap',
        '/contact', '/about', '/privacy', '/terms', '/policy',
        '/login', '/register', '/signup', '/admin', '/wp-admin',
        '/dashboard', '/profile', '/settings', '/account',
        '/press-center', '/press-service', '/press-releases',
        '/proisshestviya', '/politics', '/economy', '/sport', '/culture',
        '/world', '/russia', '/regions', '/society', '/business',
        '/science', '/technology', '/auto', '/realty', '/health'
    ]

    for pattern in exclude_patterns:
        if pattern in path:
            return False

    # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å–∞–π—Ç–æ–≤
    domain = parsed.netloc
    
    # –î–ª—è habr.com
    if 'habr.com' in domain:
        # –ü—Ä–∏–Ω–∏–º–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏ –∏ —Å—Ç–∞—Ç—å–∏
        if '/news/' in path or '/articles/' in path or '/companies/' in path:
            return True
        # –ü—Ä–∏–Ω–∏–º–∞–µ–º —Å—Å—ã–ª–∫–∏ —Å ID (–Ω–∞–ø—Ä–∏–º–µ—Ä /ru/news/990184/)
        if re.search(r'/\d+/', path):
            return True
        return False
    
    # –î–ª—è tass.ru
    if 'tass.ru' in domain:
        # –ü—Ä–∏–Ω–∏–º–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏ –≤ —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–¥–µ–ª–∞—Ö
        if '/news/' in path or '/mejdunarodnaya-panorama/' in path or '/politika/' in path or '/obschestvo/' in path:
            return True
        # –ü—Ä–∏–Ω–∏–º–∞–µ–º —Å—Å—ã–ª–∫–∏ —Å ID (–Ω–∞–ø—Ä–∏–º–µ—Ä /politika/26275619)
        if re.search(r'/\d+$', path):
            return True
        return False

    # –î–ª—è –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å–∞–π—Ç–æ–≤ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ –¥–æ–º–µ–Ω—É
    if parsed.netloc != urlparse(base_url).netloc:
        news_domains = ['tass.ru', 'ria.ru', 'interfax.ru', 'kommersant.ru', 'vedomosti.ru']
        if any(domain in parsed.netloc for domain in news_domains):
            return True
        return False

    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –¥—Ä—É–≥–∏—Ö —Å–∞–π—Ç–æ–≤
    date_pattern = r'/\d{4}/\d{1,2}/\d{1,2}/'  # /2023/12/29/
    id_pattern = r'/\d{4,}/'  # –ß–∏—Å–ª–∞ –æ—Ç 4 —Ü–∏—Ñ—Ä –∏ –±–æ–ª—å—à–µ
    news_pattern = r'/news/'  # –ü—Ä—è–º–æ–µ —É–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏
    article_pattern = r'/article/'  # –°—Ç–∞—Ç—å–∏
    story_pattern = r'/story/'  # –ò—Å—Ç–æ—Ä–∏–∏/–Ω–æ–≤–æ—Å—Ç–∏

    if (re.search(date_pattern, path) or
        re.search(id_pattern, path) or
        re.search(news_pattern, path) or
        re.search(article_pattern, path) or
        re.search(story_pattern, path)):
        return True

    # –ï—Å–ª–∏ URL –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª–∏–Ω–Ω—ã–π –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ü–∏—Ñ—Ä—ã - –≤–µ—Ä–æ—è—Ç–Ω–æ —ç—Ç–æ –Ω–æ–≤–æ—Å—Ç—å
    if len(path) > 20 and re.search(r'\d', path):
        return True

    return False

def filter_news_by_keywords(news_list: List[Tuple[str, str, str]], keywords: List[str]) -> List[Tuple[str, str, str]]:
    """–§–∏–ª—å—Ç—Ä—É–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
    if not keywords:
        return news_list

    filtered = []
    for title, url, content in news_list:
        text = f"{title} {content}".lower()
        if any(keyword.lower() in text for keyword in keywords):
            filtered.append((title, url, content))
    return filtered